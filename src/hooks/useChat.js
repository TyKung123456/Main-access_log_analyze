// src/hooks/useChat.js - Ollama Optimized
import { useState, useCallback, useEffect } from 'react';
import aiService from '../services/aiService'; // Import the aiService

export const useChat = () => {
  const [messages, setMessages] = useState([
    {
      id: 1,
      type: 'ai',
      content: '‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏ú‡∏°‡∏Ñ‡∏∑‡∏≠ AI Assistant ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Access Log \n\n‡∏ú‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Ollama Local Model ‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á:\n\nüìä ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á\nüîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥  \nüìã ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô\n‚ùì ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n\n‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö?',
      timestamp: new Date()
    }
  ]);
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState(null);
  const [modelInfo, setModelInfo] = useState({
    name: '',
    url: '',
    timeout: 30000,
    debug: false,
    availableModels: []
  });

  // Initialize model info from aiService
  useEffect(() => {
    const updateModelInfo = async () => {
      const info = aiService.getProviderInfo();
      // Ensure checkAvailability has been run to populate availableModels and ollamaModel
      await aiService.checkAvailability(); 
      const updatedInfo = aiService.getProviderInfo();
      setModelInfo({
        name: updatedInfo.ollamaModel || 'N/A',
        url: updatedInfo.ollamaUrl,
        timeout: aiService.timeout,
        debug: aiService.debug,
        availableModels: updatedInfo.availableModels.map(m => m.name)
      });
    };
    updateModelInfo();
  }, []);

  // Check Ollama availability
  const checkOllamaStatus = useCallback(async () => {
    const status = await aiService.checkAvailability();
    const info = aiService.getProviderInfo();
    setModelInfo({
      name: info.ollamaModel || 'N/A',
      url: info.ollamaUrl,
      timeout: aiService.timeout,
      debug: aiService.debug,
      availableModels: info.availableModels.map(m => m.name)
    });
    return status;
  }, []);

  // Create system prompt for Thai Access Log context
  const createSystemPrompt = (userMessage) => {
    return `‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI Assistant ‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Access Log ‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏≠‡∏≠‡∏Å‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£ ‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö:

üìä ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Access Log
üîê ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥  
üìà ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
üè¢ ‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö:
- ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: 143,898 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
- ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å: ‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏ç‡πà ‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£ 1 (‡∏ô‡∏≤‡∏ô‡∏≤‡πÄ‡∏´‡∏ô‡∏∑‡∏≠) ‡∏ä‡∏±‡πâ‡∏ô 5
- ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: EMPLOYEE, VISITOR, AFFILIATE
- ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á: IN (‡πÄ‡∏Ç‡πâ‡∏≤), OUT (‡∏≠‡∏≠‡∏Å)

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ emoji ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: ${userMessage}

‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:`;
  };

  // Send message handler with Ollama integration
  const handleSendMessage = useCallback(async (messageContent) => {
    if (!messageContent || typeof messageContent !== 'string' || !messageContent.trim()) {
      setError('‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á');
      return;
    }

    setError(null);

    const userMessage = {
      id: Date.now(),
      type: 'user',
      content: messageContent.trim(),
      timestamp: new Date()
    };

    setMessages(prev => [...prev, userMessage]);
    setIsLoading(true);

    try {
      const ollamaStatus = await checkOllamaStatus();
      
      if (!ollamaStatus.available) {
        throw new Error(`‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama ‡πÑ‡∏î‡πâ: ${ollamaStatus.error}\n\n‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Ollama ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà ${modelInfo.url}`);
      }
      
      if (!modelInfo.name) {
        throw new Error(`‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å\n\n‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà`);
      }

      if (!ollamaStatus.models.some(m => m.name === modelInfo.name)) {
        throw new Error(`‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• "${modelInfo.name}"\n\n‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà: ${ollamaStatus.models.map(m => m.name).join(', ')}\n\n‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á: ollama pull ${modelInfo.name}`);
      }

      const systemPrompt = createSystemPrompt(userMessage.content);
      const aiResponse = await aiService.generateResponse(systemPrompt);

      const aiMessage = {
        id: Date.now() + 1,
        type: 'ai',
        content: aiResponse,
        timestamp: new Date(),
        model: modelInfo.name
      };

      setMessages(prev => [...prev, aiMessage]);

    } catch (err) {
      console.error('‚ùå AI response error:', err);
      
      const errorMessage = {
        id: Date.now() + 1,
        type: 'ai',
        content: `‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: ${err.message}

üîß ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:
1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Ollama ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà: \`ollama serve\`
2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•: \`ollama list\`
3. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô: \`ollama pull ${modelInfo.name}\`
4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö URL: ${modelInfo.url}

üí° ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÉ‡∏ä‡πâ Mock AI ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå .env:
\`VITE_AI_PROVIDER=mock\``,
        timestamp: new Date(),
        isError: true
      };

      setMessages(prev => [...prev, errorMessage]);
      setError('‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠ AI Service');
    } finally {
      setIsLoading(false);
    }
  }, [modelInfo]);

  // Clear messages
  const clearMessages = useCallback(() => {
    setMessages([
      {
        id: 1,
        type: 'ai',
        content: `‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏ú‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ä‡πà‡∏ß‡∏¢‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Access Log ‡πÅ‡∏•‡πâ‡∏ß ü§ñ

üîß **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏ö‡∏ö:**
- ‡πÇ‡∏°‡πÄ‡∏î‡∏•: ${modelInfo.name}
- Ollama URL: ${modelInfo.url}
- Debug Mode: ${modelInfo.debug ? '‡πÄ‡∏õ‡∏¥‡∏î' : '‡∏õ‡∏¥‡∏î'}

‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö?`,
        timestamp: new Date()
      }
    ]);
    setError(null);
  }, [modelInfo]);

  // Clear error
  const clearError = useCallback(() => {
    setError(null);
  }, []);

  // Test Ollama connection
  const testOllamaConnection = useCallback(async () => {
    setIsLoading(true);
    try {
      const status = await checkOllamaStatus();
      const info = aiService.getProviderInfo(); // Get latest info after check
      const testMessage = {
        id: Date.now(),
        type: 'ai',
        content: `üß™ **‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Ollama:**

üì° **‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠:** ${status.available ? '‚úÖ ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à' : '‚ùå ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß'}
ü§ñ **‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:** ${info.ollamaModel || '‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å'}
üéØ **‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ:** ${info.ollamaModel && info.availableModels.some(m => m.name === info.ollamaModel) ? '‚úÖ ‡∏û‡∏£‡πâ‡∏≠‡∏°' : '‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö'}

üìã **‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö:**
${info.availableModels.length > 0 ? info.availableModels.map(model => `‚Ä¢ ${model.name}`).join('\n') : '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'}

${!status.available ? `\n‚ùó **‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:**\n1. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Ollama: \`ollama serve\`\n2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö URL: ${info.ollamaUrl}` : ''}
${status.available && info.ollamaModel && !info.availableModels.some(m => m.name === info.ollamaModel) ? `\n‚ùó **‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•:** \`ollama pull ${info.ollamaModel}\`` : ''}`,
        timestamp: new Date()
      };
      setMessages(prev => [...prev, testMessage]);
    } catch (err) {
      console.error('Connection test failed:', err);
    } finally {
      setIsLoading(false);
    }
  }, [checkOllamaStatus]);

  // Function to set the Ollama model
  const setOllamaModel = useCallback(async (modelName) => {
    const success = aiService.setOllamaModel(modelName);
    if (success) {
      await checkOllamaStatus(); // Re-check status to update modelInfo
      setMessages(prev => [...prev, {
        id: Date.now(),
        type: 'ai',
        content: `‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô **${modelName}** ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß`,
        timestamp: new Date()
      }]);
    } else {
      setMessages(prev => [...prev, {
        id: Date.now(),
        type: 'ai',
        content: `‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô **${modelName}** ‡πÑ‡∏î‡πâ`,
        timestamp: new Date(),
        isError: true
      }]);
    }
  }, [checkOllamaStatus]);

  return {
    messages,
    isLoading,
    error,
    handleSendMessage,
    clearMessages,
    clearError,
    testOllamaConnection,
    setOllamaModel, // Expose the new function
    aiProvider: 'ollama',
    modelInfo // Return the stateful modelInfo
  };
};
