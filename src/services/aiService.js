// services/aiService.js - MCP Version
class AIService {
  constructor() {
    this.provider = import.meta.env.VITE_AI_PROVIDER || 'mock'; // 'mock', 'local', or 'mcp'
    this.debug = import.meta.env.VITE_DEBUG_AI === 'true';

    // Ollama Local Model Configuration
    this.ollamaUrl = import.meta.env.VITE_OLLAMA_URL || 'http://localhost:11434';
    this.ollamaModel = import.meta.env.VITE_OLLAMA_MODEL || 'llama3.2:3b';
    this.ollamaTimeout = parseInt(import.meta.env.VITE_OLLAMA_TIMEOUT || '60000', 10);
    this.ollamaMaxRetries = parseInt(import.meta.env.VITE_OLLAMA_MAX_RETRIES || '3', 10);

    // MCP Configuration (if applicable)
    this.mcpServerName = 'ai-mcp-server';
    this.mcpToolName = 'generate_ai_response';

    this.isAvailable = null; // Cache availability status
    this.availableModels = []; // Store available models from Ollama

    if (this.debug) {
      console.log('[AI Service] Initialized with provider:', this.provider);
      console.log('[AI Service] Ollama URL:', this.ollamaUrl);
      console.log('[AI Service] Ollama Model:', this.ollamaModel);
    }
  }

  async checkAvailability() {
    if (this.provider === 'mock') {
      this.isAvailable = { available: true, provider: 'mock' };
      this.availableModels = ['mock-model']; // Provide a dummy model for mock
      return this.isAvailable;
    } else if (this.provider === 'local') {
      try {
        if (this.debug) console.log(`[AI Service] Checking Ollama availability at ${this.ollamaUrl}/api/tags`);
        const response = await fetch(`${this.ollamaUrl}/api/tags`, {
          method: 'GET',
          signal: AbortSignal.timeout(this.ollamaTimeout),
        });

        if (response.ok) {
          const data = await response.json();
          this.availableModels = data.models?.map(model => model.name) || [];
          const hasModel = this.availableModels.some(model => model === this.ollamaModel);

          if (hasModel) {
            this.isAvailable = { available: true, provider: 'local', model: this.ollamaModel, url: this.ollamaUrl };
          } else {
            this.isAvailable = { available: false, provider: 'local', error: `Model '${this.ollamaModel}' not found. Available models: ${this.availableModels.join(', ') || 'None'}`, url: this.ollamaUrl };
          }
        } else {
          this.isAvailable = { available: false, provider: 'local', error: `Ollama server responded with status ${response.status}`, url: this.ollamaUrl };
        }
      } catch (error) {
        console.error('[AI Service] Ollama connection error:', error);
        this.isAvailable = { available: false, provider: 'local', error: `Cannot connect to Ollama server: ${error.message}`, url: this.ollamaUrl };
      }
      return this.isAvailable;
    } else if (this.provider === 'mcp') {
      // In a real scenario, you might have an MCP tool to check server/model availability.
      // For now, we assume the MCP server is always available if configured.
      this.isAvailable = { available: true, provider: 'mcp' };
      this.availableModels = ['mcp-model']; // Provide a dummy model for MCP
      return this.isAvailable;
    } else {
      this.isAvailable = { available: false, error: 'Unknown provider' };
      this.availableModels = [];
      return this.isAvailable;
    }
  }

  // New: Function to upload file content for AI context
  async uploadFileForAI(file) {
    if (this.debug) {
      console.log('[AI Service] Uploading file for AI:', file.name);
    }

    try {
      const reader = new FileReader();
      const fileContentPromise = new Promise((resolve, reject) => {
        reader.onload = (event) => resolve(event.target.result);
        reader.onerror = (error) => reject(error);
        reader.readAsText(file); // Read file as text
      });

      const fileContent = await fileContentPromise;

      // In a real scenario, you might send this to a backend endpoint
      // that then makes it available to the AI model.
      // For now, we'll just return it to the frontend to be passed as context.
      if (this.debug) {
        console.log(`[AI Service] File "${file.name}" content read successfully.`);
      }
      return { success: true, fileContent: fileContent, message: 'File uploaded successfully.' };
    } catch (error) {
      console.error('[AI Service] Error reading or uploading file:', error);
      return { success: false, message: `Failed to read file: ${error.message}` };
    }
  }

  async generateResponse(userMessage, context = {}) {
    if (this.debug) {
      console.log('[AI Service] Generating response for:', userMessage.substring(0, 50) + '...');
    }

    if (!this.isAvailable || !this.isAvailable.available) {
      console.warn('[AI Service] AI service not available or not checked, using mock response.');
      // Attempt to check availability if not already checked or if it failed previously
      await this.checkAvailability();
      if (!this.isAvailable || !this.isAvailable.available) {
        throw new Error(`AI Service ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô: ${this.isAvailable?.error || 'Unknown error'}`);
      }
    }

    try {
      if (this.provider === 'local') {
        return await this.generateLocalResponse(userMessage, context);
      }
      if (this.provider === 'mcp') {
        return await this.generateMcpResponse(userMessage, context);
      }
      
      // Fallback to mock for all other providers
      return this.getMockResponse(userMessage, context);
    } catch (error) {
      console.error('[AI Service] Error generating response:', error);
      throw new Error(`Failed to generate AI response: ${error.message}`);
    }
  }

  async generateLocalResponse(userMessage, context) {
    const prompt = this.buildPrompt(userMessage, context);
    const apiUrl = `${import.meta.env.VITE_API_BASE_URL}/api/ai/chat`; // Assuming a backend endpoint for AI chat

    if (this.debug) {
      console.log('[AI Service] Calling local AI backend with prompt:', prompt);
      console.log('[AI Service] Backend API URL:', apiUrl);
    }

    try {
      const response = await fetch(apiUrl, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          prompt: prompt,
          model: this.ollamaModel,
          options: {
            temperature: 0.7,
            num_predict: 1000,
          },
        }),
        signal: AbortSignal.timeout(this.ollamaTimeout),
      });

      if (!response.ok) {
        const errorData = await response.json();
        throw new Error(errorData.message || `Backend responded with status ${response.status}`);
      }

      const data = await response.json();
      return this.processResponse(data.response);

    } catch (error) {
      console.error('[AI Service] Local AI backend call failed:', error);
      throw new Error(`Failed to connect to local AI backend: ${error.message}`);
    }
  }

  async generateMcpResponse(userMessage, context) {
    const prompt = this.buildPrompt(userMessage, context);
    
    try {
      // This is a conceptual call to an MCP tool.
      // In a real implementation, this would involve using the 'use_mcp_tool' tool.
      // For demonstration, we'll simulate a response or indicate where the call would go.
      console.log(`[AI Service] Calling MCP tool '${this.mcpToolName}' on server '${this.mcpServerName}' with prompt:`, prompt);
      
      // For now, return a mock response to avoid breaking the application without a real MCP setup
      // In a real scenario, mcpResponse would contain the AI's generated text.
      const simulatedMcpResponse = `(MCP Response) ${this.getMockResponse(userMessage, context)}`;
      return this.processResponse(simulatedMcpResponse);

    } catch (error) {
      console.warn('[AI Service] MCP tool failed, using mock response');
      throw new Error(`MCP tool failed: ${error.message}`);
    }
  }

  buildPrompt(userMessage, context) {
    const { stats = {}, fileContext = '' } = context; // New: Destructure fileContext

    let prompt = `‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Access Log`;

    if (fileContext) {
      prompt += `

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î:
\`\`\`
${fileContext}
\`\`\``;
    }

    prompt += `

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô:
- ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: ${stats.totalAccess || 0} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
- ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: ${stats.successfulAccess || 0} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á  
- ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ñ‡∏π‡∏Å‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò: ${stats.deniedAccess || 0} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
- ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥: ${stats.uniqueUsers || 0} ‡∏Ñ‡∏ô

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: ${userMessage}

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå:`;

    return prompt;
  }

  processResponse(rawResponse) {
    if (!rawResponse) return '‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ';
    
    let response = rawResponse.trim();
    response = response.replace(/^(‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:|‡∏ï‡∏≠‡∏ö:|Response:)/i, '').trim();
    
    return response;
  }

  getMockResponse(userMessage, context) {
    const { stats = {} } = context;
    const lowerMessage = userMessage.toLowerCase();

    if (lowerMessage.includes('‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥') || lowerMessage.includes('‡∏™‡∏£‡∏∏‡∏õ')) {
      return this.generateStatsResponse(stats);
    }
    
    if (lowerMessage.includes('‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò') || lowerMessage.includes('denied')) {
      return this.generateDeniedResponse(stats);
    }
    
    if (lowerMessage.includes('‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢') || lowerMessage.includes('security')) {
      return this.generateSecurityResponse(stats);
    }

    return this.generateDefaultResponse(userMessage, stats);
  }

  generateStatsResponse(stats) {
    const successRate = stats.totalAccess > 0 ? 
      ((stats.successfulAccess / stats.totalAccess) * 100).toFixed(1) : 0;
    const deniedRate = stats.totalAccess > 0 ? 
      ((stats.deniedAccess / stats.totalAccess) * 100).toFixed(1) : 0;

    return `üìä **‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏£‡∏∞‡∏ö‡∏ö**

üî¢ **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏ß‡∏°:**
‚Ä¢ ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: ${stats.totalAccess || 0} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
‚Ä¢ ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: ${stats.successfulAccess || 0} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á (${successRate}%)
‚Ä¢ ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ñ‡∏π‡∏Å‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò: ${stats.deniedAccess || 0} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á (${deniedRate}%)
‚Ä¢ ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥: ${stats.uniqueUsers || 0} ‡∏Ñ‡∏ô

üìà **‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏£‡∏∞‡∏ö‡∏ö:** ${this.getSystemStatus(stats)}

üí° **‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:** ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡∏à‡∏≥`;
  }

  generateDeniedResponse(stats) {
    const deniedRate = stats.totalAccess > 0 ? 
      ((stats.deniedAccess / stats.totalAccess) * 100).toFixed(1) : 0;

    return `üîí **‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò**

üìä **‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥:**
‚Ä¢ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò: ${stats.deniedAccess || 0} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
‚Ä¢ ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò: ${deniedRate}%

üîç **‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ:**
‚Ä¢ ‡∏ö‡∏±‡∏ï‡∏£‡∏´‡∏°‡∏î‡∏≠‡∏≤‡∏¢‡∏∏‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å
‚Ä¢ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß
‚Ä¢ ‡∏ö‡∏±‡∏ï‡∏£‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
‚Ä¢ ‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ô‡∏≠‡∏Å‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î

üí° **‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:** ${this.getDeniedRecommendation(deniedRate)}`;
  }

  generateSecurityResponse(stats) {
    const deniedRate = stats.totalAccess > 0 ? 
      (stats.deniedAccess / stats.totalAccess) * 100 : 0;
    
    let level = 'üü¢ ‡∏î‡∏µ';
    let assessment = '‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏õ‡∏Å‡∏ï‡∏¥';
    
    if (deniedRate === 0) {
      level = 'üü¢ ‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°';
      assessment = '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥';
    } else if (deniedRate > 15) {
      level = 'üî¥ ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á';
      assessment = '‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò‡∏™‡∏π‡∏á ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö';
    } else if (deniedRate > 5) {
      level = 'üü° ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á';
      assessment = '‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á ‡∏Ñ‡∏ß‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°';
    }

    return `üõ°Ô∏è **‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏£‡∏∞‡∏ö‡∏ö**

üéØ **‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢:** ${level}
üìä **‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô:** ${assessment}

üîí **‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á:**
1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á
2. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÅ‡∏ö‡∏ö Real-time
3. ‡∏ó‡∏≥ Security Audit ‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡∏à‡∏≥
4. ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Log ‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡∏à‡∏≥`;
  }

  generateDefaultResponse(userMessage, stats) {
    return `ü§ñ **‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** "${userMessage}"

üìä **‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô:**
‚Ä¢ ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: ${stats.totalAccess || 0} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
‚Ä¢ ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: ${stats.successfulAccess || 0} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
‚Ä¢ ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ñ‡∏π‡∏Å‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò: ${stats.deniedAccess || 0} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
‚Ä¢ ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥: ${stats.uniqueUsers || 0} ‡∏Ñ‡∏ô

üí° **‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:** ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢

‚ùì **‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö:** ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢, ‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò, ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°`;
  }

  getSystemStatus(stats) {
    if (stats.totalAccess === 0) return 'üì≠ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•';
    
    const successRate = (stats.successfulAccess / stats.totalAccess) * 100;
    
    if (successRate >= 95) return 'üü¢ ‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°';
    if (successRate >= 90) return 'üü° ‡∏î‡∏µ';
    if (successRate >= 80) return 'üü† ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°';
    return 'üî¥ ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö';
  }

  getDeniedRecommendation(deniedRate) {
    if (deniedRate > 20) return '‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏î‡πà‡∏ß‡∏ô ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å';
    if (deniedRate > 10) return '‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏Å‡∏•‡πâ‡∏ä‡∏¥‡∏î ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏™‡∏π‡∏á';
    if (deniedRate > 0) return '‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥';
    return '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏î‡∏µ';
  }

  getProviderInfo() {
    return {
      provider: this.provider,
      isAvailable: this.isAvailable,
      mcpServerName: this.mcpServerName,
      mcpToolName: this.mcpToolName,
      url: this.ollamaUrl, // Expose Ollama URL
      name: this.ollamaModel, // Expose current Ollama model name
      timeout: this.ollamaTimeout, // Expose Ollama timeout
      debug: this.debug, // Expose debug status
      availableModels: this.availableModels, // Expose available models
    };
  }

  setOllamaModel(modelName) {
    this.ollamaModel = modelName;
    // Reset availability to force re-check with new model
    this.isAvailable = null; 
    if (this.debug) {
      console.log(`[AI Service] Ollama model set to: ${modelName}`);
    }
  }
}

// Export ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô - ‡πÉ‡∏ä‡πâ default export
const aiService = new AIService();
export default aiService;
